{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfhe_lib.ckks.openFHE import * \n",
    "\n",
    "# === Generate Key-pairs of CKKS Context ===\n",
    "generate_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_list=[(9, 32), (32,), (32, 16), (16,), (16, 8), (8,), (8, 16), (16,), (16, 32), (32,), (32, 9), (9,)]\n",
    "\n",
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(8, activation=\"relu\")])\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(16, activation=\"relu\"),\n",
    "      layers.Dense(32, activation=\"relu\"),\n",
    "      layers.Dense(n_features, activation=\"sigmoid\")])\n",
    "    \n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "security_time=0\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, name, data_url, enc_file, n_features, iters):\n",
    "        self.id = name\n",
    "        self.enc_file = enc_file  # place wher clients save encrypted weights\n",
    "        \n",
    "        # split data into train and test\n",
    "        self.preprocessing(data_url)\n",
    "        \n",
    "        # define local training model\n",
    "        self.local_model = AnomalyDetector()\n",
    "        \n",
    "        # some helpfull stuffs\n",
    "        # self.decide_vectorized = np.vectorize(self.decide)\n",
    "        self.to_percent = lambda x: '{:.2f}%'.format(x)\n",
    "        self.num_epochs = iters\n",
    "        self.accuracies = []\n",
    "        self.losses = []\n",
    "        \n",
    "    def preprocessing(self, data_url):\n",
    "\n",
    "        dataframe = pd.read_csv(data_url, header=None)\n",
    "        print(f\"[{self.id}] Data shape: {dataframe.shape}\")\n",
    "        dataframe = dataframe.apply(pd.to_numeric, errors='coerce')\n",
    "        dataframe = dataframe.dropna()\n",
    "\n",
    "        raw_data = dataframe.values\n",
    "        labels = raw_data[:, -1]\n",
    "        data = raw_data[:, 0:-1]\n",
    "\n",
    "        self.train_data, self.test_data, self.train_labels, self.test_labels = train_test_split(data, labels, test_size=0.2, random_state=21)\n",
    "        min_val = tf.reduce_min(self.train_data)\n",
    "        max_val = tf.reduce_max(self.train_data)\n",
    "\n",
    "        self.train_data = (self.train_data - min_val) / (max_val - min_val)\n",
    "        self.test_data = (self.test_data - min_val) / (max_val - min_val)\n",
    "\n",
    "        self.train_data = tf.cast(self.train_data, tf.float32)\n",
    "        self.test_data = tf.cast(self.test_data, tf.float32)\n",
    "        self.train_labels = self.train_labels.astype(bool)\n",
    "        self.test_labels = self.test_labels.astype(bool)\n",
    "\n",
    "        self.normal_train_data = self.train_data[self.train_labels]\n",
    "        self.normal_test_data = self.test_data[self.test_labels]\n",
    "\n",
    "        self.anomalous_train_data = self.train_data[~self.train_labels]\n",
    "        self.anomalous_test_data = self.test_data[~self.test_labels]\n",
    "\n",
    "        # **Print Data Sizes for Debugging**\n",
    "        print(f\"[{self.id}] Training samples: {len(self.normal_train_data)}\")\n",
    "        print(f\"[{self.id}] Testing samples: {len(self.normal_test_data)}\")\n",
    "    \n",
    "    # def decide(self, y):\n",
    "    #     return 1. if y >=0.5 else 0.\n",
    "    \n",
    "    # def compute_accuracy(self, input, output):\n",
    "    #     prediction = self.local_model(input).data.numpy()[:, 0]\n",
    "    #     n_samples = prediction.shape[0] + 0.\n",
    "    #     prediction = self.decide_vectorized(prediction)\n",
    "    #     equal = prediction == output.data.numpy()\n",
    "    #     return 100. * equal.sum() / n_samples\n",
    "    \n",
    "    def local_training(self, debug=True):\n",
    "\n",
    "        self.local_model.compile(optimizer='adam', loss='mae', metrics=['accuracy'])\n",
    "\n",
    "        self.history=self.local_model.fit(\n",
    "            self.normal_train_data, self.normal_train_data,\n",
    "            epochs=self.num_epochs,\n",
    "            batch_size=512,\n",
    "            validation_data=(self.test_data, self.test_data),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"History keys:\", self.history.history.keys())\n",
    "\n",
    "        self.losses.extend(self.history.history['loss'])\n",
    "        self.accuracies.extend(self.history.history['accuracy'])\n",
    "    \n",
    "    def encrypted_model_params(self):\n",
    "        weights_list = self.local_model.get_weights()\n",
    "        flattened_list = [] # 1969 elements\n",
    "        for w in weights_list:\n",
    "            flattened_list.extend(w.flatten().tolist())\n",
    "\n",
    "        # print(\"Length of flattened_list:\", len(flattened_list))\n",
    "        # print(flattened_list[0:5])\n",
    "        global security_time\n",
    "        start = time.perf_counter()\n",
    "        encrypt_weights(flattened_list, self.enc_file)\n",
    "        end  = time.perf_counter()\n",
    "        security_time += end - start\n",
    "        \n",
    "    def decrypted_model_params(self):\n",
    "        global security_time\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        params = decrypt_weights(\"/enc_aggregator_weight_server.txt\")\n",
    "        end  = time.perf_counter()\n",
    "        security_time += end - start\n",
    "        \n",
    "\n",
    "        reconstructed_weights_list = []\n",
    "        start_index = 0\n",
    "\n",
    "        for shape in shapes_list:\n",
    "            size = np.prod(shape)\n",
    "            slice_ = params[start_index : start_index + size]\n",
    "            w_array = np.array(slice_).reshape(shape)\n",
    "            \n",
    "            reconstructed_weights_list.append(w_array)\n",
    "            start_index += size\n",
    "        \n",
    "        self.local_model.set_weights(reconstructed_weights_list)\n",
    "    \n",
    "    def plot_graphs(self, plot_title = 'EV PRED'):\n",
    "        plt.plot(self.losses)\n",
    "        plt.title(f\"{plot_title} - Training Loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Training Loss\")\n",
    "        plt.show()\n",
    "        plt.plot(self.accuracies)\n",
    "        plt.title(f\"{plot_title} - Training Accuracy\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Training Accuracy (Percent %)\")\n",
    "        plt.show()\n",
    "    \n",
    "    def print_result_after_training(self):\n",
    "        print('Model parameters:')\n",
    "        print('  | Weights: %s' % self.local_model.get_weights())\n",
    "        self.plot_graphs()\n",
    "    \n",
    "    # def evaluating_model(self):\n",
    "    #     test_acc = self.compute_accuracy(self.X_test, self.Y_test)\n",
    "    #     print('[+] Testing Accuracy = {}'.format(self.to_percent(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clients = [\n",
    "#     Client('Car1', 'data/cars/car_506.csv', \"/enc_weight_client1.txt\", n_features, iters=2), \n",
    "#     Client('Car2', 'data/cars/car_516.csv', \"/enc_weight_client2.txt\", n_features, iters=2),\n",
    "#     Client('Car3', 'data/cars/car_512.csv', \"/enc_weight_client3.txt\", n_features, iters=2),\n",
    "#     Client('Car4', 'data/cars/car_503.csv', \"/enc_weight_client4.txt\", n_features, iters=2)\n",
    "# ]\n",
    "\n",
    "# clients = [\n",
    "#     Client('Car1', 'data/cars/split_1.csv', \"/enc_weight_client1.txt\", n_features, iters=5), \n",
    "#     Client('Car2', 'data/cars/split_2.csv', \"/enc_weight_client2.txt\", n_features, iters=5),\n",
    "#     Client('Car3', 'data/cars/split_3.csv', \"/enc_weight_client3.txt\", n_features, iters=5),\n",
    "#     Client('Car4', 'data/cars/split_4.csv', \"/enc_weight_client4.txt\", n_features, iters=5)\n",
    "# ]\n",
    "\n",
    "clients = [\n",
    "    Client(f'Car{i+1}', f'data/cars/split_{i+1}.csv', f\"/enc_weight_client{i+1}.txt\", n_features, iters=5)\n",
    "    for i in range(5)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5 #2000\n",
    "to_percent = lambda x: '{:.2f}%'.format(x)\n",
    "n_cars = len(clients)\n",
    "n_features = 9\n",
    "    \n",
    "def compute_federated_accuracy(model, input, output):\n",
    "    prediction = model(input)\n",
    "    n_samples = prediction.shape[0]\n",
    "    s = 0.\n",
    "    for i in range(n_samples):\n",
    "        p = 1. if prediction[i] >= 0.5 else 0.\n",
    "        e = 1. if p == output[i] else 0.\n",
    "        s += e\n",
    "    return 100. * s / n_samples\n",
    "\n",
    "def federated_learning(clients):\n",
    "    # init global training model\n",
    "    # global_model = LogisticRegression(n_features)\n",
    "\n",
    "    # record losses and accuracies report from clients\n",
    "    losses = [[] for i in range(n_cars)]\n",
    "    accuracies = [[] for i in range(n_cars)]\n",
    "    \n",
    "    pbar = tqdm(range(iterations), desc='Federated Learning Process')\n",
    "    for iteration in pbar:\n",
    "        print(iteration)\n",
    "        if iteration: # enter this condition after the first iteration\n",
    "            for i in range(n_cars):\n",
    "                clients[i].decrypted_model_params()\n",
    "                # print(f\"Decrypting client {i} done\")\n",
    "        \n",
    "        for i in range(n_cars):\n",
    "            clients[i].local_training(debug=False)\n",
    "            \n",
    "            # report to server\n",
    "            losses[i].append(clients[i].losses[-1])\n",
    "            accuracies[i].append(clients[i].accuracies[-1])\n",
    "        \n",
    "        # clients encrypt the final weights of local model after training\n",
    "        for i in range(n_cars):\n",
    "            clients[i].encrypted_model_params()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            with tape.stop_recording():\n",
    "                aggregator()\n",
    "\n",
    "    \n",
    "        # logging\n",
    "        if (iteration + 1) % 100 == 0:\n",
    "            losses_str = ['{:.4f}'.format(losses[i][-1]) for i in range(n_cars)]\n",
    "            accuracies_str = [to_percent(accuracies[i][-1]) for i in range(n_cars)]\n",
    "            print('[LOG] Epoch = {0:04d}\\n> Losses = {1}\\n> Accuracies = {2}'.format(iteration + 1, losses_str, accuracies_str))\n",
    "        \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st=time.time()\n",
    "losses, accuracies = federated_learning(clients)\n",
    "et=time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_taken = et-st\n",
    "print(\"Time taken for training with ckks:\", time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_federated_graphs(diagnosis_title, losses, accuracies):\n",
    "    for i in range(n_cars):\n",
    "        plt.plot(losses[i], label=f'CAR {i+1}')\n",
    "    legend = plt.legend(loc='upper right', shadow=True)\n",
    "    plt.title(f\"{diagnosis_title} - Training Loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.show()\n",
    "    for i in range(n_cars):\n",
    "        plt.plot(accuracies[i], label=f'car {i+1}')\n",
    "    legend = plt.legend(loc='lower right', shadow=True)\n",
    "    plt.title(f\"{diagnosis_title} - Training Accuracy\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Accuracy (Percent %)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_federated_graphs('PRED EV', losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(4):\n",
    "#     local_mod = clients[i].local_model\n",
    "\n",
    "#     print(f'\\nModel parameters client{i}:')\n",
    "#     print('  | Weights: %s' % local_mod.linear.weight) ### Virtualize record of training processlobal_model.linear.weight)\n",
    "#     print('  | Bias: %s' % local_mod.linear.bias)    \n",
    "\n",
    "clients[0].decrypted_model_params()\n",
    "global_model = clients[0].local_model\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  | Weights: %s' % global_model.get_weights()) ### Virtualize record of training processlobal_model.linear.weight)\n",
    "# print('  | Bias: %s' % global_model.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare data for testing model\n",
    "# df_test = pd.read_csv('data/cars/test_data.csv')\n",
    "# df_test[\"label\"] = (df_test[\"label\"] == 10).astype(int)\n",
    "# test , X_test , Y_test  = scale_dataset(df_test , False)\n",
    "\n",
    "# test_acc = compute_federated_accuracy(global_model, X_test, Y_test)\n",
    "# print('\\nTesting Accuracy = {}'.format(to_percent(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(clients[0].normal_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_normal_train_data = tf.concat(\n",
    "    [client.normal_train_data for client in clients if len(client.normal_train_data) > 0],\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_normal_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = global_model.predict(all_normal_train_data)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, all_normal_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomalous_test_data = tf.concat(\n",
    "    [client.anomalous_test_data for client in clients if len(client.anomalous_test_data) > 0],\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = global_model.predict(all_anomalous_test_data)\n",
    "test_loss = tf.keras.losses.mae(reconstructions, all_anomalous_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(reconstructions, data)\n",
    "  return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "    # Convert tensors to NumPy arrays\n",
    "    predictions = predictions.numpy() if isinstance(predictions, tf.Tensor) else predictions\n",
    "    labels = labels.numpy() if isinstance(labels, tf.Tensor) else labels\n",
    "\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "    print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "    print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_data = tf.concat(\n",
    "    [client.test_data for client in clients if len(client.test_data) > 0],\n",
    "    axis=0\n",
    ")\n",
    "type(all_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_clients = [client for client in clients if len(client.test_labels) > 0]\n",
    "if not active_clients:\n",
    "    raise ValueError(\"No clients have test_labels to combine.\")\n",
    "test_label_arrays = [client.test_labels for client in active_clients]\n",
    "all_test_labels = np.concatenate(test_label_arrays, axis=0)\n",
    "\n",
    "# all_test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(global_model, all_test_data, threshold)\n",
    "print_stats(preds, all_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    \"\"\"\n",
    "    Evaluates classification performance and plots the ROC curve.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test (array-like): True binary labels.\n",
    "    - pred (array-like, optional): Predicted binary labels.\n",
    "    - pred_proba (array-like, optional): Predicted probabilities for the positive class.\n",
    "\n",
    "    Returns:\n",
    "    - confusion (ndarray): Confusion matrix.\n",
    "    \"\"\"\n",
    "    # Convert tensors to NumPy arrays if needed\n",
    "    y_test = y_test.numpy() if isinstance(y_test, tf.Tensor) else y_test\n",
    "    pred = pred.numpy() if isinstance(pred, tf.Tensor) else pred\n",
    "    pred_proba = pred_proba.numpy() if isinstance(pred_proba, tf.Tensor) else pred_proba\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion)\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, '\n",
    "          f'F1 Score: {f1:.4f}, AUROC: {roc_auc:.4f}')\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_proba)\n",
    "    roc_data = pd.DataFrame({'FPR': fpr, 'TPR': tpr})\n",
    "    roc_data.to_csv(\"ckks_roc.csv\", index=False)\n",
    "    print(f'FPR and TPR saved to \"ckks_roc.csv\"')\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUROC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = get_clf_eval(all_test_labels,preds,preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.set(font_scale = 2)\n",
    "sns.set_style(\"white\")\n",
    "sns.heatmap(confusion_matrix, cmap = 'gist_yarg_r',annot = True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time taken for security operations:\", security_time)\n",
    "\n",
    "print(\"Ratio :\" ,security_time/time_taken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
